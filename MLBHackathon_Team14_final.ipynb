{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Example script for Hackathon\n",
        "\n",
        "Within each cycle of active learning, you can:\n",
        "\n",
        "1. Collect training data (original training data + your query data).\n",
        "\n",
        "2. Train a prediction model to predict the DMS_score for each mutant (e.g., M0A).\n",
        "\n",
        "3. Use the trained model to predict the score for all mutant in the test set.\n",
        "\n",
        "4. Select query mutants for next round based on certain criteria. You may want to make sure you don't query the same mutant twice as you only have a limited chances of making queries in total."
      ],
      "metadata": {
        "id": "VpYk4dHcJyxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "import argparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "PDuz5mihLReY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. collect training data\n",
        "\n",
        "Upload `sequence.fasta`, `train.csv`, and `test.csv` to the current runtime:\n",
        "\n",
        "1. click the folder icon on the left\n",
        "\n",
        "2. click the upload icon and upload the files to the current directory"
      ],
      "metadata": {
        "id": "h9WI5oTTKdIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sequence.fasta', 'r') as f:\n",
        "  data = f.readlines()\n",
        "\n",
        "sequence_wt = data[1].strip()\n",
        "sequence_wt"
      ],
      "metadata": {
        "id": "Tj-TUAeZLEUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sequence_wt)"
      ],
      "metadata": {
        "id": "dewLzhLYMUSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mutated_sequence(mut, sequence_wt):\n",
        "  wt, pos, mt = mut[0], int(mut[1:-1]), mut[-1]\n",
        "\n",
        "  sequence = deepcopy(sequence_wt)\n",
        "\n",
        "  return sequence[:pos]+mt+sequence[pos+1:]"
      ],
      "metadata": {
        "id": "N-tkTaqtK9AD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZH3YKNVyR-m"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train.csv')\n",
        "df_train['sequence'] = df_train.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('test.csv')\n",
        "df_test['sequence'] = df_test.mutant.apply(lambda x: get_mutated_sequence(x, sequence_wt))\n",
        "df_test"
      ],
      "metadata": {
        "id": "tqfIASlMLQe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: integrate the query data that you acquired each round into df_train"
      ],
      "metadata": {
        "id": "B8hiStmfLXz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Train a prediction model\n",
        "\n",
        "Here, we provided a linear regression model and used one-hot encoding to encode each variant. You would need to build your own model to achieve better performances.\n",
        "\n",
        "Hint: you can perform cross-validation on the training set to evaluate your predictor before making predictions on the test set."
      ],
      "metadata": {
        "id": "2cty7BGBLdgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''hyperparameters'''\n",
        "\n",
        "seq_length = 656\n",
        "seed = 0 # seed for splitting the validation set\n",
        "val_ratio = 0.2 # proportion of validation set"
      ],
      "metadata": {
        "id": "N4rvH-HzOmN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, df, istrain=True):\n",
        "\n",
        "        alphabet = 'ACDEFGHIKLMNPQRSTVWY'\n",
        "        map_a2i = {j:i for i,j in enumerate(alphabet)}\n",
        "        map_i2a = {i:j for i,j in enumerate(alphabet)}\n",
        "\n",
        "        self.df = df\n",
        "\n",
        "        self.num_samples = len(self.df)\n",
        "        self.seq_length = len(self.df.sequence.values[0])\n",
        "        self.num_channels = 20\n",
        "\n",
        "        # TODO: replace one-hot encodings with your own encodings\n",
        "        self.encodings = np.zeros((self.num_samples, self.num_channels, self.seq_length)).astype(np.float32)\n",
        "        self.targets = np.zeros(self.num_samples).astype(np.float32)\n",
        "\n",
        "        if istrain:\n",
        "          for it, (seq,target) in enumerate(self.df[['sequence', 'DMS_score']].values):\n",
        "              for i,aa in enumerate(seq):\n",
        "                  self.encodings[it,map_a2i[aa],i] = 1\n",
        "              self.targets[it] = target\n",
        "\n",
        "          self.encodings = self.encodings.astype(np.float32)\n",
        "          self.targets = self.targets.astype(np.float32)\n",
        "        else:\n",
        "          for it, seq in enumerate(self.df['sequence'].values):\n",
        "              for i,aa in enumerate(seq):\n",
        "                  self.encodings[it,map_a2i[aa],i] = 1\n",
        "\n",
        "          self.encodings = self.encodings.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.encodings[idx]), torch.tensor(self.targets[idx])"
      ],
      "metadata": {
        "id": "bg2fQKEKLsTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ProteinDataset(df_train)\n",
        "test_dataset = ProteinDataset(df_test, istrain=False)\n",
        "\n",
        "# split validation set\n",
        "train_dataset, val_dataset = train_test_split(train_dataset, test_size=val_ratio, random_state=seed, shuffle=True)\n",
        "\n",
        "# TODO: revise according to your own model\n",
        "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)"
      ],
      "metadata": {
        "id": "UERT_WBPOgOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: build your own prediction model to replace linear regression\n",
        "# Hint: don't forget to use the validation set: you can either integrate the validation data into the training set or use it separately for early stopping\n",
        "\n",
        "X_train, y_train = next(iter(train_loader))\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train.view(X_train.size(0), -1).numpy(), y_train.numpy())\n",
        "\n",
        "X_test, _ = next(iter(test_loader))\n",
        "y_test_pred = regressor.predict(X_test.view(X_test.size(0), -1).numpy())"
      ],
      "metadata": {
        "id": "Jwv3JLn0OphU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['DMS_score_predicted'] = y_test_pred\n",
        "df_test"
      ],
      "metadata": {
        "id": "GzinalQwQhVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[['mutant', 'DMS_score_predicted']].to_csv('test_predictions.csv')"
      ],
      "metadata": {
        "id": "Yz92uKHzg0w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Select query for next round"
      ],
      "metadata": {
        "id": "TNX78OGrRZ46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.sort_values('DMS_score_predicted', ascending=False).head(100)"
      ],
      "metadata": {
        "id": "SBXWtW8zQpxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: randomly select 100 test variants to be queried.\n",
        "# Note: random selection may not be a good strategy\n",
        "# TODO: select query mutants for the next round based on your own criteria\n",
        "\n",
        "querys = np.random.choice(df_test.mutant.values, size=100, replace=False)\n",
        "querys\n"
      ],
      "metadata": {
        "id": "k46Og-FBiKyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('query.txt', 'w') as f:\n",
        "  for mutant in querys:\n",
        "    f.write(mutant+'\\n')"
      ],
      "metadata": {
        "id": "ltHYwv3Pi0lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create ESM Embeddings\n",
        "\n",
        "Create ESM embeddings for train and test data"
      ],
      "metadata": {
        "id": "E3vLR1kYHgWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fair-esm\n",
        "!pip install biopython"
      ],
      "metadata": {
        "id": "CdKqqC5HHl8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import esm\n",
        "\n",
        "def create_esm_embeddings(df,\n",
        "                          model_name=\"esm2_t33_650M_UR50D\",\n",
        "                          device='cuda:0'):\n",
        "\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model, alphabet = esm.pretrained.load_model_and_alphabet(model_name)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    batch_converter = alphabet.get_batch_converter()\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for sequence in df[\"sequence\"]:\n",
        "        data = [(\"seq\", sequence)]\n",
        "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "        batch_tokens = batch_tokens.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            results = model(batch_tokens, repr_layers=[model.num_layers], need_head_weights=False)\n",
        "\n",
        "        token_representations = results[\"representations\"][model.num_layers]\n",
        "\n",
        "        seq_representation = token_representations[0, 1 : (len(sequence) + 1)].mean(0)\n",
        "\n",
        "        embeddings.append(seq_representation.cpu().numpy())\n",
        "    df[\"esm_embedding\"] = embeddings\n",
        "    return df"
      ],
      "metadata": {
        "id": "iXO8WUTfHp_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_embeddings = create_esm_embeddings(df_train)\n",
        "df_train_embeddings.to_csv('train_embeddings.csv')"
      ],
      "metadata": {
        "id": "0CbztlkzHvjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test_embeddings = create_esm_embeddings(df_test)\n",
        "df_test_embeddings.to_csv('test_embeddings.csv')"
      ],
      "metadata": {
        "id": "na934fzuHvwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_embeddings[\"esm_embedding\"] = df_train_embeddings[\"esm_embedding\"].apply(\n",
        "    lambda emb: emb.tolist() if isinstance(emb, np.ndarray) else emb\n",
        ")\n",
        "df_test_embeddings[\"esm_embedding\"] = df_test_embeddings[\"esm_embedding\"].apply(\n",
        "    lambda emb: emb.tolist() if isinstance(emb, np.ndarray) else emb\n",
        ")\n",
        "df_train_embeddings.to_csv('train_embeddings_commas.csv')\n",
        "df_test_embeddings.to_csv('test_embeddings_commas.csv')"
      ],
      "metadata": {
        "id": "B7nzBs2MH1RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding queries to our training data\n",
        "\n",
        "Updated names of query files based on which query was being processed"
      ],
      "metadata": {
        "id": "ApTJb0jDNWoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query3_df = pd.read_csv(\"query3.txt\")\n",
        "query3_df_embeddings = create_esm_embeddings(query3_df)\n",
        "query3_df_embeddings[\"esm_embedding\"] = query3_df_embeddings[\"esm_embedding\"].apply(\n",
        "    lambda emb: emb.tolist() if isinstance(emb, np.ndarray) else emb\n",
        ")\n",
        "query3_df_embeddings.to_csv('query3_embeddings_commas.csv')\n",
        "q1q2_train_df_embeddings_commas = pd.read_csv(\"q1q2_train_embeddings_commas.csv\")\n",
        "q1q2q3_train_embeddings_commas = pd.concat([q1q2_train_df_embeddings_commas, query3_df_embeddings], ignore_index=True)\n",
        "q1q2q3_train_embeddings_commas = q1q2q3_train_embeddings_commas.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
        "q1q2q3_train_embeddings_commas.to_csv('q1q2q3_train_embeddings_commas.csv')"
      ],
      "metadata": {
        "id": "lHc3qAsRNenZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1: Multi-Head Attention Pooling + Deep MLP Regressor with Dropout and Batch Normalization + XGBoost"
      ],
      "metadata": {
        "id": "V4in-MiLIiCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import spearmanr\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "#DATASET\n",
        "class ESM_Dataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        def convert_emb(x):\n",
        "            arr = np.array(ast.literal_eval(x), dtype=np.float32)\n",
        "            if arr.ndim == 1:\n",
        "                arr = np.expand_dims(arr, axis=0)\n",
        "            return arr\n",
        "\n",
        "        self.embeddings = np.stack(df['esm_embedding'].apply(convert_emb).values)\n",
        "        self.dms_scores = df['DMS_score'].values.astype(np.float32)\n",
        "\n",
        "        N, L, D = self.embeddings.shape\n",
        "        self.embeddings = self.embeddings.reshape(N, L * D)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.embeddings = self.scaler.fit_transform(self.embeddings)\n",
        "        self.embeddings = self.embeddings.reshape(N, L, D)\n",
        "\n",
        "        self.dms_scores = (self.dms_scores - np.mean(self.dms_scores)) / np.std(self.dms_scores)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dms_scores)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        emb = self.embeddings[idx]\n",
        "        score = self.dms_scores[idx]\n",
        "        return torch.tensor(emb, dtype=torch.float32), torch.tensor(score, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# Multi-Head Attention Pooling Module (Transformer-style)\n",
        "class MultiHeadAttentionPooling(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Learns a query vector to attend over the sequence tokens.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.query = nn.Parameter(torch.randn(1, input_dim))\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len, input_dim]\n",
        "        Returns a pooled representation: [batch_size, input_dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        query = self.query.unsqueeze(0).expand(batch_size, 1, -1)\n",
        "        attn_output, _ = self.mha(query, x, x)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "        return attn_output.squeeze(1)\n",
        "\n",
        "# Deep MLP Regressor with Dropout and Batch Normalization\n",
        "class DeepMLPRegressor(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).squeeze(-1)\n",
        "\n",
        "# Multi-Head Attention Pooling + Deep MLP\n",
        "class AttnTransformerMLPRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn_pool = MultiHeadAttentionPooling(input_dim, num_heads, dropout)\n",
        "        self.deep_mlp = DeepMLPRegressor(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len, input_dim]\n",
        "        Returns: [batch_size]\n",
        "        \"\"\"\n",
        "        pooled = self.attn_pool(x)\n",
        "        return self.deep_mlp(pooled)\n",
        "\n",
        "# Training & Evaluation\n",
        "def train_model(model, train_loader, test_loader, epochs=50, lr=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x).cpu().numpy()\n",
        "            all_preds.extend(y_pred)\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    mse = np.mean((np.array(all_preds) - np.array(all_labels)) ** 2)\n",
        "    spearman_corr, _ = spearmanr(all_preds, all_labels)\n",
        "    print(f\"Test MSE: {mse:.4f}, Spearman: {spearman_corr:.4f}\")\n",
        "    return mse, spearman_corr\n",
        "\n",
        "def extract_features(model, loader):\n",
        "    \"\"\"\n",
        "    Extracts features using the multi-head attention pooling module.\n",
        "    Returns features of shape [N, input_dim] and corresponding labels.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    features_list, labels_list = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            features = model.attn_pool(x)\n",
        "            features_list.append(features.cpu().numpy())\n",
        "            labels_list.append(y.cpu().numpy())\n",
        "    features = np.concatenate(features_list, axis=0)\n",
        "    labels = np.concatenate(labels_list, axis=0)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"q1q2q3_train_embeddings_commas.csv\")\n",
        "df['esm_embedding'] = df['esm_embedding'].apply(lambda x: x if isinstance(x, list) else str(x))\n",
        "\n",
        "dataset = ESM_Dataset(df)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "input_dim = dataset.embeddings.shape[2]\n",
        "deep_model = AttnTransformerMLPRegressor(input_dim=input_dim, num_heads=4, dropout=0.1)\n",
        "print(\"Training deep attention network with multi-head pooling...\")\n",
        "mse_nn, spearman_nn = train_model(deep_model, train_loader, test_loader, epochs=100, lr=1e-4)\n",
        "\n",
        "# XGBoost\n",
        "print(\"Extracting features from attention pooling module for XGBoost...\")\n",
        "train_features, train_labels = extract_features(deep_model, train_loader)\n",
        "test_features, test_labels = extract_features(deep_model, test_loader)\n",
        "\n",
        "print(\"Training XGBoost regressor on extracted features...\")\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=200,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(train_features, train_labels)\n",
        "xgb_preds = xgb_model.predict(test_features)\n",
        "spearman_xgb, _ = spearmanr(xgb_preds, test_labels)\n",
        "mse_xgb = np.mean((xgb_preds - test_labels) ** 2)\n",
        "print(f\"XGBoost - MSE: {mse_xgb:.4f}, Spearman: {spearman_xgb:.4f}\")\n",
        "\n",
        "\n",
        "# Ensembling Neural and XGboost\n",
        "print(\"Ensembling predictions from neural network and XGBoost...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "deep_model.eval()\n",
        "all_nn_preds = []\n",
        "with torch.no_grad():\n",
        "    for x, _ in test_loader:\n",
        "        x = x.to(device)\n",
        "        preds = deep_model(x).cpu().numpy()\n",
        "        all_nn_preds.extend(preds)\n",
        "all_nn_preds = np.array(all_nn_preds)\n",
        "\n",
        "alpha = 0.5\n",
        "ensemble_preds = alpha * all_nn_preds + (1 - alpha) * xgb_preds\n",
        "spearman_ensemble, _ = spearmanr(ensemble_preds, test_labels)\n",
        "mse_ensemble = np.mean((ensemble_preds - test_labels) ** 2)\n",
        "print(f\"Ensemble - MSE: {mse_ensemble:.4f}, Spearman: {spearman_ensemble:.4f}\")"
      ],
      "metadata": {
        "id": "F4eY2M9tHv88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2: BiLSTM + Attention + MLP"
      ],
      "metadata": {
        "id": "BarT2KxUI7XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import ast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import spearmanr\n",
        "import torch.nn.functional as F\n",
        "\n",
        "df = pd.read_csv(\"/content/q1q2q3_train_embeddings_commas.csv\")\n",
        "df[\"esm_embedding\"] = df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "X = torch.tensor(df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "y = torch.tensor(df[\"DMS_score\"].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "X = (X - X.mean(dim=0)) / (X.std(dim=0) + 1e-8)\n",
        "\n",
        "class DMSDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# DataLoader\n",
        "train_size = int(0.8 * len(df))\n",
        "test_size = len(df) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(DMSDataset(X, y), [train_size, test_size])\n",
        "\n",
        "# Best hyperparameters from tuning\n",
        "best_params = {\n",
        "    \"hidden_dim\": 512,\n",
        "    \"lstm_layers\": 3,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"dropout\": 0.3\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# BiLSTM + Attention + MLP\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout=0.3):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, lstm_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        output = self.fc(attn_out)\n",
        "        return output\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model = BiLSTMRegressor(input_dim=X.shape[1], **best_params).to(device)\n",
        "model = BiLSTMRegressor(\n",
        "    input_dim=X.shape[1],\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    lstm_layers=best_params[\"lstm_layers\"],\n",
        "    dropout=best_params[\"dropout\"]\n",
        ").to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=best_params[\"learning_rate\"], weight_decay=best_params[\"weight_decay\"])\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, train_loader, test_loader, epochs=50):\n",
        "    best_spearman = -1\n",
        "    best_model_path = \"best_model.pth\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                preds = model(X_batch).cpu().numpy().flatten()\n",
        "                targets = y_batch.cpu().numpy().flatten()\n",
        "                all_preds.extend(preds)\n",
        "                all_targets.extend(targets)\n",
        "\n",
        "        spearman_corr, _ = spearmanr(all_preds, all_targets)\n",
        "\n",
        "        # Save best model\n",
        "        if spearman_corr > best_spearman:\n",
        "            best_spearman = spearman_corr\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Saved new best model at epoch {epoch+1} with Spearman: {spearman_corr:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f} - Spearman: {spearman_corr:.4f}\")\n",
        "\n",
        "    print(f\"Training complete! Best Spearman: {best_spearman:.4f}\")\n",
        "\n",
        "# Train the final model\n",
        "train_model(model, train_loader, test_loader, epochs=50)\n"
      ],
      "metadata": {
        "id": "tUIGQtXkHwIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3: BiLSTM + Attention + MLP + Pathogenicity data"
      ],
      "metadata": {
        "id": "vZXLP0htO42j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import ast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import spearmanr\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# dataset + Pathogenicity\n",
        "df = pd.read_csv(\"/content/q1q2q3_train_embeddings_commas.csv\")\n",
        "df[\"esm_embedding\"] = df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "patho_df = pd.read_csv(\"AF-P42898-F1-hg38.csv\")\n",
        "patho_df = patho_df[[\"protein_variant\", \"am_pathogenicity\"]]\n",
        "\n",
        "df = df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "df[\"am_pathogenicity\"] = df[\"am_pathogenicity\"].fillna(0.0)\n",
        "\n",
        "X_embed = torch.tensor(df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "patho_tensor = torch.tensor(df[\"am_pathogenicity\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X = torch.cat([X_embed, patho_tensor], dim=1)\n",
        "y = torch.tensor(df[\"DMS_score\"].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "X = (X - X.mean(dim=0)) / (X.std(dim=0) + 1e-8)\n",
        "\n",
        "\n",
        "class DMSDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "test_size = len(df) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(DMSDataset(X, y), [train_size, test_size])\n",
        "\n",
        "best_params = {\n",
        "    \"hidden_dim\": 512,\n",
        "    \"lstm_layers\": 3,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"weight_decay\": 0.001,\n",
        "    \"dropout\": 0.3\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=best_params[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# BiLSTM + Attention + MLP\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout=0.3):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, lstm_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        output = self.fc(attn_out)\n",
        "        return output\n",
        "\n",
        "# Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BiLSTMRegressor(\n",
        "    input_dim=X.shape[1],\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    lstm_layers=best_params[\"lstm_layers\"],\n",
        "    dropout=best_params[\"dropout\"]\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=best_params[\"learning_rate\"],\n",
        "                        weight_decay=best_params[\"weight_decay\"])\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=50):\n",
        "    best_spearman = -1\n",
        "    best_model_path = \"best_model.pth\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                preds = model(X_batch).cpu().numpy().flatten()\n",
        "                targets = y_batch.cpu().numpy().flatten()\n",
        "                all_preds.extend(preds)\n",
        "                all_targets.extend(targets)\n",
        "\n",
        "        spearman_corr, _ = spearmanr(all_preds, all_targets)\n",
        "\n",
        "        if spearman_corr > best_spearman:\n",
        "            best_spearman = spearman_corr\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\" Saved new best model at epoch {epoch+1} with Spearman: {spearman_corr:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f} - Spearman: {spearman_corr:.4f}\")\n",
        "\n",
        "    print(f\" Training complete! Best Spearman: {best_spearman:.4f}\")\n",
        "    print(f\" Best model saved as '{best_model_path}'\")\n",
        "\n",
        "train_model(model, train_loader, test_loader, epochs=50)"
      ],
      "metadata": {
        "id": "DBmq3Fw-IP8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 4: BiLSTM + Attention + MLP + Pathogenicity data + foldX data as Parameter"
      ],
      "metadata": {
        "id": "95_2aohdPLgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "vRt9UueGenCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import ast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import spearmanr\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Load Data\n",
        "embedding_file = \"q1q2q3_train_embeddings_commas.csv\"\n",
        "patho_file = \"AF-P42898-F1-hg38.csv\"\n",
        "ddg_file = \"mutant_ddG.csv\"\n",
        "\n",
        "df = pd.read_csv(embedding_file)\n",
        "df[\"esm_embedding\"] = df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "patho_df = pd.read_csv(patho_file)[[\"protein_variant\", \"am_pathogenicity\"]].drop_duplicates(\"protein_variant\")\n",
        "df = df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "df[\"am_pathogenicity\"] = df[\"am_pathogenicity\"].fillna(0.0)\n",
        "\n",
        "# Attach FoldX ddG values\n",
        "ddg_df = pd.read_csv(ddg_file)\n",
        "df = df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "df[\"ddG\"] = df[\"ddG\"].fillna(0.0)\n",
        "\n",
        "\n",
        "X_embed = torch.tensor(df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "patho_tensor = torch.tensor(df[\"am_pathogenicity\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "ddg_tensor = torch.tensor(df[\"ddG\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "y = torch.tensor(df[\"DMS_score\"].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "X_embed = (X_embed - X_embed.mean(dim=0)) / (X_embed.std(dim=0) + 1e-8)\n",
        "X = torch.cat([X_embed, patho_tensor, ddg_tensor], dim=1)\n",
        "\n",
        "class DMSDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "test_size = len(df) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(DMSDataset(X, y), [train_size, test_size])\n",
        "\n",
        "params = {\n",
        "    \"hidden_dim\": 512,\n",
        "    \"lstm_layers\": 3,\n",
        "    \"dropout\": 0.3,\n",
        "    \"batch_size\": 64,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"weight_decay\": 0.001\n",
        "}\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# BiLSTM + Attention + MLP\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout=0.3):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, lstm_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "        return self.fc(attn_out)\n",
        "\n",
        "# Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BiLSTMRegressor(\n",
        "    input_dim=X.shape[1],\n",
        "    hidden_dim=params[\"hidden_dim\"],\n",
        "    lstm_layers=params[\"lstm_layers\"],\n",
        "    dropout=params[\"dropout\"]\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=50):\n",
        "    best_spearman = -1\n",
        "    best_model_path = \"best_model.pth\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                preds = model(X_batch).cpu().numpy().flatten()\n",
        "                targets = y_batch.cpu().numpy().flatten()\n",
        "                all_preds.extend(preds)\n",
        "                all_targets.extend(targets)\n",
        "\n",
        "        spearman_corr, _ = spearmanr(all_preds, all_targets)\n",
        "\n",
        "        if spearman_corr > best_spearman:\n",
        "            best_spearman = spearman_corr\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\" Saved new best model at epoch {epoch+1} with Spearman: {spearman_corr:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f} - Spearman: {spearman_corr:.4f}\")\n",
        "\n",
        "    print(f\" Training complete! Best Spearman: {best_spearman:.4f}\")\n",
        "    print(f\" Best model saved as '{best_model_path}'\")\n",
        "\n",
        "\n",
        "train_model(model, train_loader, test_loader, epochs=50)\n"
      ],
      "metadata": {
        "id": "BUPDir8MIQQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "fMEXyOCUeoyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "\n",
        "# BiLSTMRegressor Model\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout=0.3):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, lstm_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "        return self.fc(attn_out)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "test_file = \"test_embeddings_commas.csv\"\n",
        "train_file = \"q1q2q3_train_embeddings_commas.csv\"\n",
        "patho_file = \"AF-P42898-F1-hg38.csv\"\n",
        "ddg_file = \"mutant_ddG.csv\"\n",
        "model_file = \"best_model.pth\"\n",
        "output_csv = \"predictions.csv\"\n",
        "top10_txt = \"top10_mutations.txt\"\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "train_df[\"esm_embedding\"] = train_df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "X_train = torch.tensor(train_df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "mean, std = X_train.mean(dim=0), X_train.std(dim=0)\n",
        "\n",
        "test_df = pd.read_csv(test_file)\n",
        "test_df[\"esm_embedding\"] = test_df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "# Attach pathogenicity\n",
        "patho_df = pd.read_csv(patho_file)[[\"protein_variant\", \"am_pathogenicity\"]].drop_duplicates(\"protein_variant\")\n",
        "test_df = test_df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "test_df[\"am_pathogenicity\"] = test_df[\"am_pathogenicity\"].fillna(0.0)\n",
        "\n",
        "# Attach FoldX ddG values\n",
        "ddg_df = pd.read_csv(ddg_file)\n",
        "test_df = test_df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "test_df[\"ddG\"] = test_df[\"ddG\"].fillna(0.0)\n",
        "\n",
        "esm_tensor = torch.tensor(test_df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "esm_tensor = (esm_tensor - mean) / (std + 1e-8)\n",
        "patho_tensor = torch.tensor(test_df[\"am_pathogenicity\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "ddg_tensor = torch.tensor(test_df[\"ddG\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test = torch.cat([esm_tensor, patho_tensor, ddg_tensor], dim=1)\n",
        "\n",
        "\n",
        "best_params = {\n",
        "    \"hidden_dim\": 512,\n",
        "    \"lstm_layers\": 3,\n",
        "    \"dropout\": 0.3\n",
        "}\n",
        "\n",
        "model = BiLSTMRegressor(\n",
        "    input_dim=X_test.shape[1],\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    lstm_layers=best_params[\"lstm_layers\"],\n",
        "    dropout=best_params[\"dropout\"]\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(model_file, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test.to(device)).cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "test_df[\"DMS_score_predicted\"] = preds\n",
        "test_df[[\"mutant\", \"DMS_score_predicted\"]].to_csv(output_csv, index=False)\n",
        "print(f\"Saved predictions to {output_csv}\")\n",
        "\n",
        "top10 = test_df.nlargest(10, \"DMS_score_predicted\")[[\"mutant\", \"DMS_score_predicted\"]]\n",
        "with open(top10_txt, \"w\") as f:\n",
        "    f.write(\"Effectiveness of the Top 10 Mutation Predictions (Recall-Based Evaluation)\\n\")\n",
        "    f.write(top10.to_string(index=False))\n",
        "\n",
        "print(f\"Saved top 10 mutations to {top10_txt}\")\n"
      ],
      "metadata": {
        "id": "pvR72lNaerw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pred_df = pd.read_csv(\"predictions.csv\")\n",
        "\n",
        "pred_df = pred_df.drop_duplicates(subset=\"mutant\", keep=\"first\")\n",
        "\n",
        "pred_df.to_csv(\"predictions_deduplicated.csv\", index=False)\n",
        "print(\"Deduplicated file saved as 'predictions_parameter.csv'\")"
      ],
      "metadata": {
        "id": "DDGwe0wIesje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 5: BiLSTM + Attention + MLP + Pathogenicity data + foldX data as Reinforcement (Reward Shaping)"
      ],
      "metadata": {
        "id": "sdFD_e7SPNDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "AV1L_WE0ewvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import ast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import spearmanr\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load Data\n",
        "embedding_file = \"q1q2q3_train_embeddings_commas.csv\"\n",
        "patho_file = \"AF-P42898-F1-hg38.csv\"\n",
        "ddg_file = \"mutant_ddG.csv\"\n",
        "\n",
        "df = pd.read_csv(embedding_file)\n",
        "df[\"esm_embedding\"] = df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "# Attach pathogenicity\n",
        "patho_df = pd.read_csv(patho_file)[[\"protein_variant\", \"am_pathogenicity\"]].drop_duplicates(\"protein_variant\")\n",
        "df = df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "df[\"am_pathogenicity\"] = df[\"am_pathogenicity\"].fillna(0.0)\n",
        "\n",
        "# Attach FoldX ddG values (used as reward signal)\n",
        "ddg_df = pd.read_csv(ddg_file)\n",
        "df = df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "df[\"ddG\"] = df[\"ddG\"].fillna(0.0)\n",
        "\n",
        "\n",
        "X_embed = torch.tensor(df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "X_embed = (X_embed - X_embed.mean(dim=0)) / (X_embed.std(dim=0) + 1e-8)\n",
        "\n",
        "patho_tensor = torch.tensor(df[\"am_pathogenicity\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "X = torch.cat([X_embed, patho_tensor], dim=1)\n",
        "\n",
        "y = torch.tensor(df[\"DMS_score\"].values, dtype=torch.float32).view(-1, 1)\n",
        "rewards = torch.tensor(df[\"ddG\"].values, dtype=torch.float32)\n",
        "\n",
        "def ddg_to_reward(ddg):\n",
        "    return torch.sigmoid(-ddg)\n",
        "\n",
        "reward_tensor = ddg_to_reward(rewards)\n",
        "\n",
        "\n",
        "class DMSDataset(Dataset):\n",
        "    def __init__(self, X, y, reward):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.reward = reward\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], self.reward[idx]\n",
        "\n",
        "train_size = int(0.8 * len(df))\n",
        "test_size = len(df) - train_size\n",
        "dataset = DMSDataset(X, y, reward_tensor)\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# BiLSTMRegressor Model\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout=0.3):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, lstm_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "        return self.fc(attn_out)\n",
        "\n",
        "# Training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BiLSTMRegressor(input_dim=X.shape[1], hidden_dim=512, lstm_layers=3, dropout=0.3).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
        "base_criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=50):\n",
        "    best_spearman = -1\n",
        "    best_model_path = \"best_model_rl.pth\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for X_batch, y_batch, r_batch in train_loader:\n",
        "            X_batch, y_batch, r_batch = X_batch.to(device), y_batch.to(device), r_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            base_loss = base_criterion(y_pred, y_batch)\n",
        "\n",
        "\n",
        "            shaped_loss = (base_loss * (1 - r_batch.unsqueeze(1))).mean()\n",
        "            shaped_loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += shaped_loss.item()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        all_preds, all_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, _ in test_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                preds = model(X_batch).cpu().numpy().flatten()\n",
        "                targets = y_batch.cpu().numpy().flatten()\n",
        "                all_preds.extend(preds)\n",
        "                all_targets.extend(targets)\n",
        "\n",
        "        spearman_corr, _ = spearmanr(all_preds, all_targets)\n",
        "        if spearman_corr > best_spearman:\n",
        "            best_spearman = spearman_corr\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Epoch {epoch+1}: New best Spearman {spearman_corr:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - RL Loss: {total_loss/len(train_loader):.4f} - Spearman: {spearman_corr:.4f}\")\n",
        "\n",
        "    print(f\"Training complete! Best Spearman: {best_spearman:.4f}\")\n",
        "\n",
        "\n",
        "train_model(model, train_loader, test_loader, epochs=50)\n"
      ],
      "metadata": {
        "id": "QKY3b54BIQdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "EuFBizugeyTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# BiLSTMRegressor\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout=0.3):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, lstm_layers,\n",
        "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "        return self.fc(attn_out)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "test_file = \"test_embeddings_commas.csv\"\n",
        "patho_file = \"AF-P42898-F1-hg38.csv\"\n",
        "ddg_file = \"mutant_ddG.csv\"\n",
        "train_file = \"q1q2q3_train_embeddings_commas.csv\"\n",
        "model_file = \"best_model_rl.pth\"\n",
        "output_csv = \"predictions_rl.csv\"\n",
        "top10_txt = \"top10_mutations_rl.txt\"\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "train_df[\"esm_embedding\"] = train_df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "X_train = torch.tensor(train_df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "mean, std = X_train.mean(dim=0), X_train.std(dim=0)\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(test_file)\n",
        "test_df[\"esm_embedding\"] = test_df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "# Attach pathogenicity\n",
        "patho_df = pd.read_csv(patho_file)[[\"protein_variant\", \"am_pathogenicity\"]].drop_duplicates(\"protein_variant\")\n",
        "test_df = test_df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "test_df[\"am_pathogenicity\"] = test_df[\"am_pathogenicity\"].fillna(0.0)\n",
        "\n",
        "# Attach ddG\n",
        "ddg_df = pd.read_csv(ddg_file)\n",
        "test_df = test_df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "test_df[\"ddG\"] = test_df[\"ddG\"].fillna(0.0)\n",
        "\n",
        "esm_tensor = torch.tensor(test_df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "esm_tensor = (esm_tensor - mean) / (std + 1e-8)\n",
        "patho_tensor = torch.tensor(test_df[\"am_pathogenicity\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test = torch.cat([esm_tensor, patho_tensor], dim=1)\n",
        "\n",
        "\n",
        "model = BiLSTMRegressor(\n",
        "    input_dim=X_test.shape[1],\n",
        "    hidden_dim=512,\n",
        "    lstm_layers=3,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(model_file, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test.to(device)).cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "test_df[\"DMS_score_predicted\"] = preds\n",
        "test_df[[\"mutant\", \"DMS_score_predicted\", \"ddG\"]].to_csv(output_csv, index=False)\n",
        "print(f\"Saved predictions to {output_csv}\")\n",
        "\n",
        "\n",
        "top10 = test_df.nlargest(10, \"DMS_score_predicted\")[[\"mutant\", \"DMS_score_predicted\", \"ddG\"]]\n",
        "with open(top10_txt, \"w\") as f:\n",
        "    f.write(\"Top 10 Predicted Mutants (High Functional Score)\\n\")\n",
        "    f.write(top10.to_string(index=False))\n",
        "\n",
        "print(f\"Saved top 10 mutations to {top10_txt}\")\n"
      ],
      "metadata": {
        "id": "HM4MesE8e03Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pred_df = pd.read_csv(\"predictions.csv\")\n",
        "\n",
        "pred_df = pred_df.drop_duplicates(subset=\"mutant\", keep=\"first\")\n",
        "\n",
        "pred_df.to_csv(\"predictions_deduplicated.csv\", index=False)\n",
        "print(\"Deduplicated file saved as 'predictions_reinforcement.csv'\")"
      ],
      "metadata": {
        "id": "wAOzxukee1s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 6: BiLSTM + Attention + MLP + Pathogenicity data + foldX data as both Parameter & Reinforcement (Reward Shaping)"
      ],
      "metadata": {
        "id": "Go5n5wzePO3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "jJNFiX84e7du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import ast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-3\n",
        "EPOCHS = 50\n",
        "HIDDEN_DIM = 512\n",
        "LSTM_LAYERS = 3\n",
        "DROPOUT = 0.3\n",
        "\n",
        "EMBEDDING_FILE = \"q1q2q3_train_embeddings_commas.csv\"\n",
        "PATHO_FILE = \"AF-P42898-F1-hg38.csv\"\n",
        "DDG_FILE = \"mutant_ddG.csv\"\n",
        "BEST_MODEL_PATH = \"best_model_combined.pth\"\n",
        "\n",
        "df = pd.read_csv(EMBEDDING_FILE)\n",
        "df[\"esm_embedding\"] = df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "# Attach pathogenicity\n",
        "patho_df = pd.read_csv(PATHO_FILE)[[\"protein_variant\", \"am_pathogenicity\"]].drop_duplicates(\"protein_variant\")\n",
        "df = df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "df[\"am_pathogenicity\"] = df[\"am_pathogenicity\"].fillna(0.0)\n",
        "\n",
        "# Attach FoldX ddG\n",
        "ddg_df = pd.read_csv(DDG_FILE)\n",
        "df = df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "df[\"ddG\"] = df[\"ddG\"].fillna(0.0)\n",
        "\n",
        "\n",
        "# ESM embeddings\n",
        "X_embed = torch.tensor(df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "\n",
        "X_mean, X_std = X_embed.mean(dim=0), X_embed.std(dim=0)\n",
        "X_embed = (X_embed - X_mean) / (X_std + 1e-8)\n",
        "\n",
        "patho_tensor = torch.tensor(df[\"am_pathogenicity\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "ddg_tensor = torch.tensor(df[\"ddG\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X = torch.cat([X_embed, patho_tensor, ddg_tensor], dim=1)\n",
        "\n",
        "y = torch.tensor(df[\"DMS_score\"].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "def ddg_to_reward(ddg_values):\n",
        "    return torch.sigmoid(-ddg_values)\n",
        "\n",
        "reward_tensor = ddg_to_reward(ddg_tensor.squeeze(1))\n",
        "\n",
        "\n",
        "class DMSDataset(Dataset):\n",
        "    def __init__(self, X, y, rewards):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.rewards = rewards\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx], self.rewards[idx]\n",
        "\n",
        "full_dataset = DMSDataset(X, y, reward_tensor)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# BiLSTMRegressor\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout=0.3):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        return self.fc(attn_out)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BiLSTMRegressor(\n",
        "    input_dim=X.shape[1],\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    lstm_layers=LSTM_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "\n",
        "base_criterion = nn.MSELoss(reduction=\"none\")\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "# Training Loop (with reward shaping)\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=EPOCHS):\n",
        "    best_spearman = -1.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for X_batch, y_batch, r_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            r_batch = r_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch).squeeze(1)\n",
        "            base_loss = base_criterion(y_pred, y_batch.squeeze(1))\n",
        "            shaped_loss = (base_loss * (1.0 - r_batch)).mean()\n",
        "\n",
        "            shaped_loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += shaped_loss.item()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_targets = []\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, _ in test_loader:\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "                preds = model(X_batch).cpu().numpy().flatten()\n",
        "                targets = y_batch.cpu().numpy().flatten()\n",
        "                all_preds.extend(preds)\n",
        "                all_targets.extend(targets)\n",
        "\n",
        "        spearman_corr, _ = spearmanr(all_preds, all_targets)\n",
        "\n",
        "        if spearman_corr > best_spearman:\n",
        "            best_spearman = spearman_corr\n",
        "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "            print(f\"Epoch {epoch+1}: New best Spearman = {spearman_corr:.4f}\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | RL Loss: {total_loss/len(train_loader):.4f} | Spearman: {spearman_corr:.4f}\")\n",
        "\n",
        "    print(f\"Training complete! Best Spearman: {best_spearman:.4f}\")\n",
        "\n",
        "train_model(model, train_loader, test_loader)\n"
      ],
      "metadata": {
        "id": "rzw4AyRUIQuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "-s0qx8mCe-x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "\n",
        "# Same BiLSTMRegressor\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lstm_layers, dropout=0.3):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
        "            elif 'bias' in name:\n",
        "                nn.init.zeros_(param)\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "        return self.fc(attn_out)\n",
        "\n",
        "TEST_EMBEDDING_FILE = \"test_embeddings_commas.csv\"\n",
        "PATHO_FILE = \"AF-P42898-F1-hg38.csv\"\n",
        "DDG_FILE = \"mutant_ddG.csv\"\n",
        "TRAIN_EMBEDDING_FILE = \"q1q2q3_train_embeddings_commas.csv\"\n",
        "BEST_MODEL_PATH = \"best_model_combined.pth\"\n",
        "PREDICTION_CSV = \"predictions_combined.csv\"\n",
        "TOP10_TXT = \"top10_mutations_combined.txt\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import ast\n",
        "train_df = pd.read_csv(TRAIN_EMBEDDING_FILE)\n",
        "train_df[\"esm_embedding\"] = train_df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "X_train = torch.tensor(train_df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "X_mean, X_std = X_train.mean(dim=0), X_train.std(dim=0)\n",
        "\n",
        "test_df = pd.read_csv(TEST_EMBEDDING_FILE)\n",
        "test_df[\"esm_embedding\"] = test_df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "# Attach pathogenicity\n",
        "patho_df = pd.read_csv(PATHO_FILE)[[\"protein_variant\", \"am_pathogenicity\"]].drop_duplicates(\"protein_variant\")\n",
        "test_df = test_df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "test_df[\"am_pathogenicity\"] = test_df[\"am_pathogenicity\"].fillna(0.0)\n",
        "\n",
        "# Attach ddG\n",
        "ddg_df = pd.read_csv(DDG_FILE)\n",
        "test_df = test_df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "test_df[\"ddG\"] = test_df[\"ddG\"].fillna(0.0)\n",
        "\n",
        "esm_test = torch.tensor(test_df[\"esm_embedding\"].tolist(), dtype=torch.float32)\n",
        "esm_test = (esm_test - X_mean) / (X_std + 1e-8)\n",
        "patho_test = torch.tensor(test_df[\"am_pathogenicity\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "ddg_test = torch.tensor(test_df[\"ddG\"].values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test = torch.cat([esm_test, patho_test, ddg_test], dim=1)\n",
        "\n",
        "\n",
        "model = BiLSTMRegressor(\n",
        "    input_dim=X_test.shape[1],\n",
        "    hidden_dim=512,\n",
        "    lstm_layers=3,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test.to(device)).cpu().numpy().flatten()\n",
        "\n",
        "test_df[\"DMS_score_predicted\"] = preds\n",
        "test_df[[\"mutant\", \"DMS_score_predicted\", \"ddG\"]].to_csv(PREDICTION_CSV, index=False)\n",
        "print(f\"Predictions saved to '{PREDICTION_CSV}'\")\n",
        "\n",
        "test_df.drop_duplicates(subset=\"mutant\", keep=\"first\", inplace=True)\n",
        "\n",
        "top10 = test_df.nlargest(10, \"DMS_score_predicted\")[[\"mutant\", \"DMS_score_predicted\", \"ddG\"]]\n",
        "with open(TOP10_TXT, \"w\") as f:\n",
        "    f.write(\"Top 10 Predicted Mutants (Combined Approach)\\n\\n\")\n",
        "    f.write(top10.to_string(index=False))\n",
        "print(f\"Top 10 mutations saved to '{TOP10_TXT}'\")\n"
      ],
      "metadata": {
        "id": "ydBccQOpIQ7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model 7: BiLSTM + Attention + MLP + Active learning while using Pathogenicity data + foldX data"
      ],
      "metadata": {
        "id": "gz5klUQEfd93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"q1q2q3_train_embeddings_commas.csv\"\n",
        "test_file  = \"test_embeddings_commas.csv\"\n",
        "ddg_file   = \"mutant_ddG_ALL.csv\"\n",
        "patho_file = \"immunogenicity.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_file)\n",
        "# train_df[\"esm_embedding\"] = train_df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "ddg_df = pd.read_csv(ddg_file)\n",
        "def remove_second_char_if_A(s):\n",
        "    if len(s) > 2 and s[1] == 'A':\n",
        "        return s[0] + s[2:]\n",
        "    else:\n",
        "        return s\n",
        "ddg_df[\"mutant\"] = ddg_df[\"mutant\"].apply(remove_second_char_if_A)\n",
        "\n",
        "patho_df = pd.read_csv(patho_file)\n",
        "patho_df[\"protein_variant\"] = patho_df[\"protein_variant\"].apply(remove_second_char_if_A)\n",
        "\n",
        "# Merge into train\n",
        "train_df = train_df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "train_df = train_df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "train_df[\"pathogenicity_score\"] = train_df[\"pathogenicity_score\"].fillna(0.0)\n",
        "print(train_df)"
      ],
      "metadata": {
        "id": "BEFTalb3fdtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pandas as pd\n",
        "import ast\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# BiLSTM Model Definition\n",
        "class BiLSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_layers=2, dropout=0.2):\n",
        "        super(BiLSTMRegressor, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.attn = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for name, param in self.lstm.named_parameters():\n",
        "            if \"weight\" in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "            elif \"bias\" in name:\n",
        "                nn.init.zeros_(param)\n",
        "        for name, param in self.fc.named_parameters():\n",
        "            if \"weight\" in name:\n",
        "                nn.init.kaiming_uniform_(param, nonlinearity=\"relu\")\n",
        "            elif \"bias\" in name:\n",
        "                nn.init.zeros_(param)\n",
        "        nn.init.xavier_uniform_(self.attn.weight)\n",
        "        nn.init.zeros_(self.attn.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.unsqueeze(1)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_weights = torch.softmax(self.attn(lstm_out).squeeze(-1), dim=1)\n",
        "        attn_out = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
        "        return self.fc(attn_out)\n",
        "\n",
        "\n",
        "class DMSTrainDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def train_model(model, train_loader, val_loader=None, epochs=20, lr=1e-4, wd=1e-4, device=\"cpu\"):\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    best_spearman = -1\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(X_batch).squeeze(1)\n",
        "            loss = criterion(preds, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Evaluate\n",
        "        spearman_corr = 0.0\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            all_preds, all_targets = [], []\n",
        "            with torch.no_grad():\n",
        "                for X_val, y_val in val_loader:\n",
        "                    X_val = X_val.to(device)\n",
        "                    y_val = y_val.to(device)\n",
        "                    val_preds = model(X_val).cpu().numpy().flatten()\n",
        "                    val_targets = y_val.cpu().numpy().flatten()\n",
        "                    all_preds.extend(val_preds)\n",
        "                    all_targets.extend(val_targets)\n",
        "\n",
        "            spearman_corr, _ = spearmanr(all_preds, all_targets)\n",
        "            if spearman_corr > best_spearman:\n",
        "                best_spearman = spearman_corr\n",
        "\n",
        "        print(f\"[Epoch {epoch}/{epochs}] Loss={avg_loss:.4f}, Spearman={spearman_corr:.4f}\")\n",
        "\n",
        "    print(f\"Training complete. Best Spearman on val set: {best_spearman:.4f}\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "R43mIr2ZgY1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Active Learning\n",
        "def main_active_learning_loop(num_iterations=5):\n",
        "\n",
        "    train_file = \"q1q2q3_train_embeddings_commas.csv\"\n",
        "    test_file  = \"test_embeddings_commas.csv\"\n",
        "    ddg_file   = \"mutant_ddG_ALL.csv\"\n",
        "    patho_file = \"immunogenicity.csv\"\n",
        "\n",
        "\n",
        "    train_df = pd.read_csv(train_file)\n",
        "    train_df[\"esm_embedding\"] = train_df[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "    test_df_full = pd.read_csv(test_file)\n",
        "    test_df_full[\"esm_embedding\"] = test_df_full[\"esm_embedding\"].apply(ast.literal_eval)\n",
        "\n",
        "    unlabeled_pool_df = test_df_full.copy()\n",
        "\n",
        "    ddg_df = pd.read_csv(ddg_file)\n",
        "    def remove_second_char_if_A(s):\n",
        "        if len(s) > 2 and s[1] == 'A':\n",
        "            return s[0] + s[2:]\n",
        "        else:\n",
        "            return s\n",
        "    ddg_df[\"mutant\"] = ddg_df[\"mutant\"].apply(remove_second_char_if_A)\n",
        "\n",
        "    patho_df = pd.read_csv(patho_file)\n",
        "    patho_df[\"protein_variant\"] = patho_df[\"protein_variant\"].apply(remove_second_char_if_A)\n",
        "\n",
        "    # Merge into train\n",
        "    train_df = train_df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "    train_df = train_df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "    train_df[\"pathogenicity_score\"] = train_df[\"pathogenicity_score\"].fillna(0.0)\n",
        "\n",
        "    # Merge into unlabeled pool\n",
        "    unlabeled_pool_df = unlabeled_pool_df.merge(ddg_df, on=\"mutant\", how=\"left\")\n",
        "    unlabeled_pool_df = unlabeled_pool_df.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\")\n",
        "    unlabeled_pool_df[\"pathogenicity_score\"] = unlabeled_pool_df[\"pathogenicity_score\"].fillna(0.0)\n",
        "    print(train_df)\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    for _, row in train_df.iterrows():\n",
        "        emb = row[\"esm_embedding\"]\n",
        "        ddg_val   = row[\"ddG\"] if pd.notna(row[\"ddG\"]) else 0.0\n",
        "        patho_val = row[\"pathogenicity_score\"] if pd.notna(row[\"pathogenicity_score\"]) else 0.0\n",
        "        full_input = emb + [ddg_val, patho_val]\n",
        "        X_list.append(full_input)\n",
        "        y_list.append(row[\"DMS_score\"])\n",
        "\n",
        "    X_data = torch.tensor(X_list, dtype=torch.float32)\n",
        "    y_data = torch.tensor(y_list, dtype=torch.float32)\n",
        "\n",
        "    X_mean = X_data.mean(dim=0)\n",
        "    X_std  = X_data.std(dim=0) + 1e-8\n",
        "    X_data = (X_data - X_mean) / X_std\n",
        "\n",
        "    full_dataset = DMSTrainDataset(X_data, y_data)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size   = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False)\n",
        "\n",
        "    input_dim = X_data.shape[1]\n",
        "    model = BiLSTMRegressor(input_dim=input_dim, hidden_dim=256, num_layers=2, dropout=0.2)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    # Active Learning Loop\n",
        "    for iteration in range(1, num_iterations+1):\n",
        "        print(f\"\\n================== AL Iteration {iteration}/{num_iterations} ==================\\n\")\n",
        "\n",
        "        model = train_model(model, train_loader, val_loader, epochs=10, lr=1e-4, wd=1e-4, device=device)\n",
        "\n",
        "        if len(unlabeled_pool_df) == 0:\n",
        "            print(\"No more unlabeled data to pick from.\")\n",
        "            break\n",
        "\n",
        "        X_unlab_list = []\n",
        "        for _, row in unlabeled_pool_df.iterrows():\n",
        "            emb = row[\"esm_embedding\"]\n",
        "            ddg_val   = row[\"ddG\"] if pd.notna(row[\"ddG\"]) else 0.0\n",
        "            patho_val = row[\"pathogenicity_score\"] if pd.notna(row[\"pathogenicity_score\"]) else 0.0\n",
        "            full_input = emb + [ddg_val, patho_val]\n",
        "            X_unlab_list.append(full_input)\n",
        "\n",
        "        X_unlab_tensor = torch.tensor(X_unlab_list, dtype=torch.float32)\n",
        "        X_unlab_tensor = (X_unlab_tensor - X_mean) / X_std\n",
        "\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for i in range(len(X_unlab_tensor)):\n",
        "                x_i = X_unlab_tensor[i].unsqueeze(0).to(device)\n",
        "                y_hat = model(x_i).item()\n",
        "                preds.append(y_hat)\n",
        "\n",
        "        unlabeled_pool_df[\"DMS_score_predicted\"] = preds\n",
        "\n",
        "\n",
        "        top200 = unlabeled_pool_df.nlargest(200, \"DMS_score_predicted\") if len(unlabeled_pool_df) >= 200 else unlabeled_pool_df\n",
        "        top10  = top200.nsmallest(10, \"ddG\") if len(top200) >= 10 else top200\n",
        "\n",
        "        print(f\"  => Selected {len(top10)} new mutants for labeling (out of {len(top200)} top).\")\n",
        "\n",
        "        true_labels_for_top10 = np.random.uniform(0.0, 1.0, size=len(top10))\n",
        "        top10[\"DMS_score\"] = true_labels_for_top10\n",
        "\n",
        "        train_df = pd.concat([train_df, top10], ignore_index=True)\n",
        "\n",
        "        top10_mutants = set(top10[\"mutant\"].unique())\n",
        "        unlabeled_pool_df = unlabeled_pool_df[~unlabeled_pool_df[\"mutant\"].isin(top10_mutants)].reset_index(drop=True)\n",
        "\n",
        "        X_new_list, y_new_list = [], []\n",
        "        for _, row in train_df.iterrows():\n",
        "            emb = row[\"esm_embedding\"]\n",
        "            if isinstance(emb, str):\n",
        "                emb = ast.literal_eval(emb)\n",
        "            ddg_val   = row[\"ddG\"] if pd.notna(row[\"ddG\"]) else 0.0\n",
        "            patho_val = row[\"pathogenicity_score\"] if pd.notna(row[\"pathogenicity_score\"]) else 0.0\n",
        "            full_input = emb + [ddg_val, patho_val]\n",
        "            X_new_list.append(full_input)\n",
        "            y_new_list.append(row[\"DMS_score\"])\n",
        "\n",
        "        X_new_tensor = torch.tensor(X_new_list, dtype=torch.float32)\n",
        "        X_new_tensor = (X_new_tensor - X_mean) / X_std\n",
        "        y_new_tensor = torch.tensor(y_new_list, dtype=torch.float32)\n",
        "\n",
        "        new_dataset = DMSTrainDataset(X_new_tensor, y_new_tensor)\n",
        "        train_loader = DataLoader(new_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    print(f\"\\nAll {num_iterations} active learning iterations complete!\\n\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    X_test_full_list = []\n",
        "    for _, row in test_df_full.iterrows():\n",
        "        emb = row[\"esm_embedding\"]\n",
        "        ddg_val   = row[\"ddG\"] if \"ddG\" in row and pd.notna(row[\"ddG\"]) else 0.0\n",
        "        patho_val = row[\"pathogenicity_score\"] if \"pathogenicity_score\" in row and pd.notna(row[\"pathogenicity_score\"]) else 0.0\n",
        "        full_input = emb + [ddg_val, patho_val]\n",
        "        X_test_full_list.append(full_input)\n",
        "\n",
        "    X_test_full_tensor = torch.tensor(X_test_full_list, dtype=torch.float32)\n",
        "\n",
        "    test_df_full_merged = test_df_full.copy()\n",
        "    test_df_full_merged = test_df_full_merged.merge(ddg_df, on=\"mutant\", how=\"left\", suffixes=(\"\", \"_ddg\"))\n",
        "    test_df_full_merged = test_df_full_merged.merge(patho_df, left_on=\"mutant\", right_on=\"protein_variant\", how=\"left\", suffixes=(\"\", \"_patho\"))\n",
        "    test_df_full_merged[\"ddG\"] = test_df_full_merged[\"ddG\"].fillna(0.0)\n",
        "    test_df_full_merged[\"pathogenicity_score\"] = test_df_full_merged[\"pathogenicity_score\"].fillna(0.0)\n",
        "\n",
        "\n",
        "    X_test_full_list = []\n",
        "    for _, row in test_df_full_merged.iterrows():\n",
        "        emb = row[\"esm_embedding\"]\n",
        "        if isinstance(emb, str):\n",
        "            emb = ast.literal_eval(emb)\n",
        "        ddg_val   = row[\"ddG\"] if pd.notna(row[\"ddG\"]) else 0.0\n",
        "        patho_val = row[\"pathogenicity_score\"] if pd.notna(row[\"pathogenicity_score\"]) else 0.0\n",
        "        full_input = emb + [ddg_val, patho_val]\n",
        "        X_test_full_list.append(full_input)\n",
        "\n",
        "    X_test_full_tensor = torch.tensor(X_test_full_list, dtype=torch.float32)\n",
        "    X_test_full_tensor = (X_test_full_tensor - X_mean) / X_std\n",
        "\n",
        "    preds_full = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(X_test_full_tensor)):\n",
        "            x_i = X_test_full_tensor[i].unsqueeze(0).to(device)\n",
        "            y_hat = model(x_i).item()\n",
        "            preds_full.append(y_hat)\n",
        "\n",
        "    test_df_full_merged[\"DMS_score_predicted\"] = preds_full\n",
        "\n",
        "    test_df_full_merged[[\"mutant\", \"DMS_score_predicted\", \"ddG\", \"pathogenicity_score\"]].to_csv(\n",
        "        \"final_test_predictions.csv\", index=False)\n",
        "    print(\"Saved final test predictions (all test mutants) to 'final_test_predictions.csv'\")\n",
        "\n",
        "    final_top10 = test_df_full_merged.nlargest(10, \"DMS_score_predicted\")\n",
        "    with open(\"final_top10_mutations.txt\", \"w\") as f:\n",
        "        f.write(\"Top 10 Mutants by Final Predicted DMS Score (Among Entire Test Set)\\n\\n\")\n",
        "        f.write(final_top10[[\"mutant\", \"DMS_score_predicted\", \"ddG\", \"pathogenicity_score\"]]\n",
        "                .to_string(index=False))\n",
        "    print(\"Saved final top 10 mutants to 'final_top10_mutations.txt'\")\n",
        "\n",
        "def main():\n",
        "    main_active_learning_loop(num_iterations=5)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "lf4IGU6ggkWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing correlation between ddG and DMS"
      ],
      "metadata": {
        "id": "q-pWNWbXfvcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "\n",
        "subset_df = train_df.dropna(subset=[\"ddG\", \"DMS_score\"])\n",
        "\n",
        "ddg_vals = subset_df[\"ddG\"].values\n",
        "dms_vals = subset_df[\"DMS_score\"].values\n",
        "\n",
        "# 2) Spearman correlation\n",
        "spearman_corr, spearman_p = spearmanr(ddg_vals, dms_vals)\n",
        "print(f\"Spearman correlation = {spearman_corr:.4f}, p-value = {spearman_p:.4e}\")\n",
        "\n",
        "# 3) Pearson correlation\n",
        "pearson_corr, pearson_p = pearsonr(ddg_vals, dms_vals)\n",
        "print(f\"Pearson correlation = {pearson_corr:.4f}, p-value = {pearson_p:.4e}\")\n"
      ],
      "metadata": {
        "id": "ODpDtT2Df26K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}